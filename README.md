# GraCoRe
GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models
Abstract:
Evaluating the graph comprehension and rea-
soning abilities of Large Language Models
(LLMs) is challenging and often incomplete.
Existing benchmarks focus primarily on pure
graph understanding, lacking a comprehensive
evaluation across all graph types and detailed
capability definitions. This paper presents Gra-
CoRe, a benchmark for systematically assess-
ing LLMsâ€™ graph comprehension and reason-
ing. GraCoRe uses a three-tier hierarchical tax-
onomy to categorize and test models on pure
graph and heterogeneous graphs, subdividing
capabilities into 10 distinct areas tested through
19 tasks. Our benchmark includes 11 datasets
with 5,140 graphs of varying complexity. We
evaluated three closed-source and seven open-
source LLMs, conducting thorough analyses
from both ability and task perspectives. Key
findings reveal that semantic enrichment en-
hances reasoning performance, node ordering
impacts task success, and the ability to process
longer texts does not necessarily improve graph
comprehension or reasoning.
